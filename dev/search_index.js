var documenterSearchIndex = {"docs":
[{"location":"constructors/poisson/#Poisson-EPCA","page":"Poisson","title":"Poisson EPCA","text":"","category":"section"},{"location":"constructors/poisson/","page":"Poisson","title":"Poisson","text":"Name PoissonEPCA\nG(theta) e^theta\ng(theta) e^theta\nmu Space[1] positive\nTheta Space real\nAppropriate Data count, probability","category":"page"},{"location":"constructors/poisson/","page":"Poisson","title":"Poisson","text":"Poisson EPCA minimizes the generalized KL divergence making it well-suited for compressing probability profiles. Poisson EPCA has also been used in reinforcement learning to solve partially observed Markov decision processes (POMDPs) with belief compression (Roy et al., 2005). ","category":"page"},{"location":"constructors/poisson/","page":"Poisson","title":"Poisson","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/poisson/#Documentation","page":"Poisson","title":"Documentation","text":"","category":"section"},{"location":"constructors/poisson/#ExpFamilyPCA.PoissonEPCA","page":"Poisson","title":"ExpFamilyPCA.PoissonEPCA","text":"PoissonEPCA(indim::Integer, outdim::Integer; options::Options = Options())\n\nPoisson EPCA.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\noptions::Options: Optional parameters.\n\nReturns\n\nepca: An EPCA subtype for the Poisson distribution.\n\n\n\n\n\n","category":"function"},{"location":"math/references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"math/references/","page":"References","title":"References","text":"Azoury, K. S. and Warmuth, M. K. (2001). Relative Loss Bounds for On-Line Density Estimation with the Exponential Family of Distributions. Machine learning 43, 211–246.\n\n\n\nBoyd, S. and Vandenberghe, L. (2004). Convex Optimization (Cambridge University Press).\n\n\n\nBregman, L. (1967). The relaxation method of finding the common point of convex sets and its application to the solution of problems in convex programming. USSR Computational Mathematics and Mathematical Physics 7, 200–217.\n\n\n\nCollins, M.; Dasgupta, S. and Schapire, R. E. (2001). A Generalization of Principal Components Analysis to the Exponential Family. Advances in Neural Information Processing Systems 14.\n\n\n\nForster, J. and Warmuth, M. K. (2002). Relative Expected Instantaneous Loss Bounds. Journal of Computer and System Sciences 64, 76–102.\n\n\n\nGowda, S.; Ma, Y.; Cheli, A.; Gwóźzdź, M.; Shah, V. B.; Edelman, A. and Rackauckas, C. (2022). High-Performance Symbolic-Numerics via Multiple Dispatch. ACM Commun. Comput. Algebra 55, 92–96.\n\n\n\nGreenacre, M. (2021). Compositional data analysis. Annual Review of Statistics and its Application 8, 271–299.\n\n\n\nHastie, T.; Tibshirani, R.; Friedman, J. H. and Friedman, J. H. (2009). The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Vol. 2 (Springer).\n\n\n\nItakura, F. and Saito, S. (1968). Analysis Synthesis Telephony Based on the Maximum Likelihood Method. In: Proceedings of the 6th International Congress on Acoustics (IEEE, Los Alamitos, CA); p. C–17–C–20.\n\n\n\nKarpinski, S. (2019). The Unreasonable Effectiveness of Multiple Dispatch. Conference Talk at JuliaCon 2019, available at https://www.youtube.com/watch?v=kc9HwsxE1OY. Accessed: 2024-09-13.\n\n\n\nMcCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models. 2 Edition, Chapman & Hall/CRC Monographs on Statistics and Applied                Probability (Chapman & Hall/CRC, Philadelphia, PA).\n\n\n\nMogensen, P. K. and Riseth, A. N. (2018). Optim: A mathematical optimization package for Julia. Journal of Open Source Software 3, 615.\n\n\n\nPearson, K. (1901). On Lines and Planes of Closest Fit to Systems of Points in Space. Philosophical Magazine 2, 559–572.\n\n\n\nRoy, N.; Gordon, G. and Thrun, S. (2005). Finding Approximate POMDP solutions Through Belief Compression. Journal of Artificial Intelligence Research 23, 1–40.\n\n\n\n","category":"page"},{"location":"constructors/gaussian/#Gaussian-EPCA","page":"Gaussian","title":"Gaussian EPCA","text":"","category":"section"},{"location":"constructors/gaussian/","page":"Gaussian","title":"Gaussian","text":"Name GaussianEPCA or NormalEPCA\nG(theta) theta^2  2\ng(theta) theta\nmu Space[1] real\nTheta Space real\nAppropriate Data continuous","category":"page"},{"location":"constructors/gaussian/","page":"Gaussian","title":"Gaussian","text":"The Gaussian EPCA objective is equivalent to regular PCA.","category":"page"},{"location":"constructors/gaussian/","page":"Gaussian","title":"Gaussian","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/gaussian/#Documentation","page":"Gaussian","title":"Documentation","text":"","category":"section"},{"location":"constructors/gaussian/#ExpFamilyPCA.GaussianEPCA","page":"Gaussian","title":"ExpFamilyPCA.GaussianEPCA","text":"Alias for NormalEPCA.\n\n\n\n\n\n","category":"function"},{"location":"constructors/gaussian/#ExpFamilyPCA.NormalEPCA","page":"Gaussian","title":"ExpFamilyPCA.NormalEPCA","text":"GaussianEPCA(indim::Integer, outdim::Integer; options::Options = Options())\n\nAn EPCA model with Gaussian loss.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\noptions::Options: Optional parameters.\n\nReturns\n\nepca: An EPCA subtype for the Gaussian distribution.\n\n\n\n\n\n","category":"function"},{"location":"constructors/weibull/#Weibull-EPCA","page":"Weibull","title":"Weibull EPCA","text":"","category":"section"},{"location":"constructors/weibull/","page":"Weibull","title":"Weibull","text":"Name WeibullEPCA\nG(theta) -log(-theta) - log k\ng(theta) -frac1theta\nmu Space[1] mathbbR   0 \nTheta Space negative\nAppropriate Data nonnegative continuous","category":"page"},{"location":"constructors/weibull/","page":"Weibull","title":"Weibull","text":"WeibullEPCA omits it the known shape parameter k since it does not affect the Weibull EPCA objective.","category":"page"},{"location":"constructors/weibull/","page":"Weibull","title":"Weibull","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/weibull/#Documentation","page":"Weibull","title":"Documentation","text":"","category":"section"},{"location":"constructors/weibull/#ExpFamilyPCA.WeibullEPCA","page":"Weibull","title":"ExpFamilyPCA.WeibullEPCA","text":"WeibullEPCA(indim::Integer, outdim::Integer; options::Options = Options(A_init_value = -1, A_upper = -eps(), V_lower = eps()))\n\nWeibull EPCA.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\noptions::Options: Optional parameters for model initialization. Default NegativeDomain().\n\nReturns\n\nepca: An EPCA subtype for the Weibull distribution.\n\n\n\n\n\n","category":"function"},{"location":"math/appendix/inverses/#The-Inverse-of-the-Link-Function-is-the-Gradient-of-the-Convex-Conjugate","page":"Inverse Link Functions","title":"The Inverse of the Link Function is the Gradient of the Convex Conjugate","text":"","category":"section"},{"location":"math/appendix/inverses/","page":"Inverse Link Functions","title":"Inverse Link Functions","text":"Observe that the gradient of the dual is the inverse of the gradient of the log-partition,","category":"page"},{"location":"math/appendix/inverses/","page":"Inverse Link Functions","title":"Inverse Link Functions","text":"beginaligned\nf(g(theta)) \n= f(mu) \n= nabla_mu F(mu) \n= nabla_mu Big mu theta - G(theta)Big \n= theta + mu nabla_mu theta - g(theta) nabla_mu theta \n= theta \nendaligned","category":"page"},{"location":"math/appendix/inverses/","page":"Inverse Link Functions","title":"Inverse Link Functions","text":"The converse is similar, so f = g^-1.","category":"page"},{"location":"math/appendix/inverses/#Remark","page":"Inverse Link Functions","title":"Remark","text":"","category":"section"},{"location":"math/appendix/inverses/","page":"Inverse Link Functions","title":"Inverse Link Functions","text":"Since f is the inverse link function g^-1 and mu = g(theta), we also have f(mu) = theta.","category":"page"},{"location":"math/appendix/poisson/#Poisson-EPCA-and-the-Generalized-KL-Divergence","page":"Poisson EPCA and Generalized KL-Divergence","title":"Poisson EPCA and the Generalized KL-Divergence","text":"","category":"section"},{"location":"math/appendix/poisson/","page":"Poisson EPCA and Generalized KL-Divergence","title":"Poisson EPCA and Generalized KL-Divergence","text":"The cumulant of the Poisson distribution is G(theta) = e^theta, so the the link function (its derivative) is g(theta) = e^theta. From the appendix, we know that f(x) = g^-1(x) = -logx and ","category":"page"},{"location":"math/appendix/poisson/","page":"Poisson EPCA and Generalized KL-Divergence","title":"Poisson EPCA and Generalized KL-Divergence","text":"beginaligned\nF(x) \n= theta cdot x - G(theta) \n= f(x) cdot x - G(f(x)) \n= x logx - x\nendaligned","category":"page"},{"location":"math/appendix/poisson/","page":"Poisson EPCA and Generalized KL-Divergence","title":"Poisson EPCA and Generalized KL-Divergence","text":"The Bregman divergence induced from F is","category":"page"},{"location":"math/appendix/poisson/","page":"Poisson EPCA and Generalized KL-Divergence","title":"Poisson EPCA and Generalized KL-Divergence","text":"beginaligned\nB_F(p  q) \n= F(p) - F(q) - langle f(q) p - q rangle \n= p log p - p - q log q + q - langle log q p - q rangle \n= p log fracpq - p + q\nendaligned","category":"page"},{"location":"math/appendix/poisson/","page":"Poisson EPCA and Generalized KL-Divergence","title":"Poisson EPCA and Generalized KL-Divergence","text":"so B_F is generalized KL-divergence.","category":"page"},{"location":"math/bregman/#Bregman-Divergences","page":"Bregman Divergences","title":"Bregman Divergences","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"The EPCA objective is formulated as a Bregman divergence (Bregman, 1967). Bregman divergences are a measure of difference between two points (often probability distributions); however, they are not proper metrics, because they do not always satisfy symmetry and the triangle inequality.","category":"page"},{"location":"math/bregman/#Definition","page":"Bregman Divergences","title":"Definition","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"Formally, the Bregman divergence B_F associated with a function F(theta) is defined as","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"B_F(p  q) = F(p) - F(q) - langle f(p) p - q rangle","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"where ","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"F(mu) is a strictly convex and continuously differentiable function, \nf(mu) = nabla_mu F(mu) is the gradient of F, \nand langle cdot cdot rangle denotes an inner product.","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"Intuitively, the Bregman divergence expresses the difference at p between F and its first-order Taylor expansion about q.","category":"page"},{"location":"math/bregman/#Aside:-Properties","page":"Bregman Divergences","title":"Aside: Properties","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"Bregman divergences vanish B_F(p  q) = 0 if and only if their inputs also vanish p = q = 0. They are also always non-negative B_F(p  q) geq 0 for all p q in mathrmdomain(F).","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"info: Info\nWhile the full EPCA objective is always non-negative, the EPCA loss may be negative because ExpFamilyPCA.jl uses transformed objectives that are equivalent but not equal to minimizing a sum of Bregman divergences.  ","category":"page"},{"location":"math/bregman/#The-Exponential-Family","page":"Bregman Divergences","title":"The Exponential Family","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"The natural exponential family is a broad class of probability distributions that includes many common distributions such as the Gaussian, binomial, Poisson, and gamma distributions. A probability distribution belongs to the exponential family if its probability density function (or mass function for discrete variables) can be expressed in the following canonical form:","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"p_theta(x) = h(x) exp(langle theta x rangle - G(theta) )","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"where","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"theta is the natural parameter (also called the canonical parameter) of the distribution,\nh(x) is the base measure, independent of theta,\nand G(theta) is the log-partition function (also called the cumulant function) defined as:","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"G(theta) = log int h(x) exp(langle theta x rangle)  dx","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"The log-partition function G(theta) ensures that the probability distribution integrates (or sums) to 1.","category":"page"},{"location":"math/bregman/#Key-Parameters","page":"Bregman Divergences","title":"Key Parameters","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"The natural parameter theta controls the distribution’s shape in its canonical form. For example, the natural parameter for the Poisson distribution is log lambda.\nThe expectation parameter mu is the expected value of the sufficient statistic,[1] computed as the mean of the data under the distribution. In exponential family distributions, it is related to the natural parameter through the link function g:","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"mu = mathbbE_thetaX = nabla_theta G(theta) = g(theta)","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"where E_theta is the expectation with respect to the distribution p_theta (see appendix). Similarly, we also have theta = f(mu) (see appendix).","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"[1]: The sufficient statistic for the natural exponential family is simply the identity.","category":"page"},{"location":"math/bregman/#Convex-Conjugation","page":"Bregman Divergences","title":"Convex Conjugation","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"The fact that f and g are inverses follows from the stronger claim that F and G are convex conjugates. For a convex function F, its convex conjugate (or dual)[2] F^* is","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"F^*(theta) = sup_mu langle theta mu rangle - F(mu)","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"Convex conjugation is also an involution meaning it inverts itself, so F^** = F. Conjugation provides a rich structure for converting between natural and expectation parameters and, as we explain in the next section, helps induce multiple useful specifications of the EPCA objective.","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"[2]: Duality also refers to the concept in convex analysis (Boyd and Vandenberghe, 2004).","category":"page"},{"location":"math/bregman/#Bregman-Loss-Functions","page":"Bregman Divergences","title":"Bregman Loss Functions","text":"","category":"section"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"Bregman divergences are crucial to EPCA, because they are equivalent (up to a constant) to maximum likelihood estimation for the exponential family (Azoury and Warmuth, 2001; Forster and Warmuth, 2002). To see this, consider the negative log-likelihood of such a distribution:","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"-ell(x theta) = G(theta) - langle x theta rangle","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"We want to show that this is equivalent to the Bregman divergence B_F(x  mu). From the previous subsection, we know G is the convex conjugate of F, so:","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"G(theta) = langle theta mu rangle - F(mu)","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"Substituting G back into the negative log-likelihood, then yields:","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"beginaligned\n-ell(x theta) = (langle theta mu rangle - F(mu)) - langle x theta rangle \n= -F(mu) - langle theta x - mu rangle\nendaligned","category":"page"},{"location":"math/bregman/","page":"Bregman Divergences","title":"Bregman Divergences","text":"The last line is equivalent to B_F(x  mu) up to a constant, meaning we can interpret the Bregman divergence as a loss function that generalizes the maximum likelihood of any exponential family distribution. In the next section, we expand this idea to derive specific constructors for the EPCA objective.","category":"page"},{"location":"constructors/continuous_bernoulli/#Continuous-Bernoulli-EPCA","page":"Continuous Bernoulli","title":"Continuous Bernoulli EPCA","text":"","category":"section"},{"location":"constructors/continuous_bernoulli/#Math","page":"Continuous Bernoulli","title":"Math","text":"","category":"section"},{"location":"constructors/continuous_bernoulli/","page":"Continuous Bernoulli","title":"Continuous Bernoulli","text":"Name ContinuousBernoulliEPCA\nG(theta) log frace^theta - 1theta\ng(theta) fractheta-1theta + frac1e^theta - 1\nmu Space[1] (0 1) setminus frac12\nTheta Space mathbbR setminus  0 \nAppropriate Data unit interval","category":"page"},{"location":"constructors/continuous_bernoulli/","page":"Continuous Bernoulli","title":"Continuous Bernoulli","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/continuous_bernoulli/#Documentation","page":"Continuous Bernoulli","title":"Documentation","text":"","category":"section"},{"location":"constructors/continuous_bernoulli/#ExpFamilyPCA.ContinuousBernoulliEPCA","page":"Continuous Bernoulli","title":"ExpFamilyPCA.ContinuousBernoulliEPCA","text":"ContinuousBernoulliEPCA(indim::Integer, outdim::Integer; options::Options = Options(μ = 0.5))\n\nContinuous Bernoulli EPCA.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\noptions::Options: Optional parameters (default: μ = 0.25).\n\nReturns\n\nepca: An EPCA subtype for the continuous Bernoulli distribution.\n\n\n\n\n\n","category":"function"},{"location":"math/appendix/gaussian/#Gaussian-EPCA-and-the-Squared-Frobenius-Norm","page":"Gaussian EPCA and the Squared Frobenius Norm","title":"Gaussian EPCA and the Squared Frobenius Norm","text":"","category":"section"},{"location":"math/appendix/gaussian/","page":"Gaussian EPCA and the Squared Frobenius Norm","title":"Gaussian EPCA and the Squared Frobenius Norm","text":"We want to show that the squared Frobenius norm frac12 A - B _F^2 is a Bregman divergence. Let psi(A) = frac12A_F^2, so that nabla psi(A) = A. Using norm properties, we can then write the Bregman divergence associated with psi as","category":"page"},{"location":"math/appendix/gaussian/","page":"Gaussian EPCA and the Squared Frobenius Norm","title":"Gaussian EPCA and the Squared Frobenius Norm","text":"beginaligned\nB_psi(A  B) = psi(A) - psi(B) - langle nabla psi(B) A - B rangle \n= frac12A_F^2 - frac12B_F^2 - langle B A rangle + langle B B rangle \n= frac12A_F^2 - langle B A rangle + frac12B_F^2 \n= frac12 big langle A A rangle - 2langle B A rangle + langle B B rangle big \n= frac12 langle A - B A - B rangle \n= frac12  A - B _F^2\nendaligned","category":"page"},{"location":"math/appendix/gaussian/","page":"Gaussian EPCA and the Squared Frobenius Norm","title":"Gaussian EPCA and the Squared Frobenius Norm","text":"Similarly, the Bregman divergence induced from the log-partition of the Gaussian G(theta) = theta^22 is the squared Euclidean distance. Thus, EPCA generalizes PCA.","category":"page"},{"location":"constructors/gamma/#Gamma-EPCA","page":"Gamma","title":"Gamma EPCA","text":"","category":"section"},{"location":"constructors/gamma/#Math","page":"Gamma","title":"Math","text":"","category":"section"},{"location":"constructors/gamma/","page":"Gamma","title":"Gamma","text":"Name GammaEPCA or ItakuraSaitoEPCA\nG(theta) -log(-theta)\ng(theta) -frac1theta\nmu Space[1] mathbbR setminus  0 \nTheta Space negative\nAppropriate Data positive","category":"page"},{"location":"constructors/gamma/","page":"Gamma","title":"Gamma","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/gamma/#Documentation","page":"Gamma","title":"Documentation","text":"","category":"section"},{"location":"constructors/gamma/#ExpFamilyPCA.GammaEPCA","page":"Gamma","title":"ExpFamilyPCA.GammaEPCA","text":"GammaEPCA(indim::Integer, outdim::Integer; options::Options = Options(A_init_value = -2, A_upper = -eps(), V_lower = eps()))\n\nGamma EPCA.\n\nArguments\n\nindim::Integer: The dimension of the input space.\noutdim::Integer: The dimension of the latent (output) space.\noptions::Options: Optional configuration parameters for the EPCA model. \nA_init_value: Initial fill value for matrix A (default: -2).\nA_upper: The upper bound for the matrix A, default is -eps().\nV_lower: The lower bound for the matrix V, default is eps().\n\ntip: Tip\nTry using options = NegativeDomain() if you encounter domain errors when calling fit! or compress.\n\nReturns\n\nepca: An instance of an EPCA subtype.\n\n\n\n\n\n","category":"function"},{"location":"constructors/gamma/#ExpFamilyPCA.ItakuraSaitoEPCA","page":"Gamma","title":"ExpFamilyPCA.ItakuraSaitoEPCA","text":"Alias for GammaEPCA.\n\n\n\n\n\n","category":"function"},{"location":"constructors/binomial/#Binomial-EPCA","page":"Binomial","title":"Binomial EPCA","text":"","category":"section"},{"location":"constructors/binomial/#Math","page":"Binomial","title":"Math","text":"","category":"section"},{"location":"constructors/binomial/","page":"Binomial","title":"Binomial","text":"Name BinomialEPCA\nG(theta) n log(1 + e^theta)\ng(theta) fracn e^theta1+e^theta\nmu Space[1] (0 n)\nTheta Space real\nAppropriate Data count\nn n geq 0 (number of trials)","category":"page"},{"location":"constructors/binomial/","page":"Binomial","title":"Binomial","text":"G is the scaled softplus function and g is the scaled logistic function.","category":"page"},{"location":"constructors/binomial/","page":"Binomial","title":"Binomial","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/binomial/#Documentation","page":"Binomial","title":"Documentation","text":"","category":"section"},{"location":"constructors/binomial/#ExpFamilyPCA.BinomialEPCA","page":"Binomial","title":"ExpFamilyPCA.BinomialEPCA","text":"BinomialEPCA(indim::Integer, outdim::Integer, n::Integer; options::Options = Options(μ = 0.5))\n\nBinomial EPCA.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\nn::Integer: A known parameter representing the number of trials (nonnegative).\noptions::Options: Optional parameters (default: μ = 0.5).\n\nReturns\n\nepca: An EPCA subtype for the binomial distribution.\n\n\n\n\n\n","category":"function"},{"location":"constructors/negative_binomial/#Negative-Binomial-EPCA","page":"Negative Binomial","title":"Negative Binomial EPCA","text":"","category":"section"},{"location":"constructors/negative_binomial/","page":"Negative Binomial","title":"Negative Binomial","text":"Name NegativeBinomialEPCA\nG(theta) -r log(1 - e^theta)\ng(theta) - fracr  e^theta1 - e^theta\nmu Space[1] positive\nTheta Space negative\nAppropriate Data count\nr r  0 (number of successes)","category":"page"},{"location":"constructors/negative_binomial/","page":"Negative Binomial","title":"Negative Binomial","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/negative_binomial/#Documentation","page":"Negative Binomial","title":"Documentation","text":"","category":"section"},{"location":"constructors/negative_binomial/#ExpFamilyPCA.NegativeBinomialEPCA","page":"Negative Binomial","title":"ExpFamilyPCA.NegativeBinomialEPCA","text":"NegativeBinomialEPCA(indim::Integer, outdim::Integer, r::Integer; options::Options = Options(A_init_value = -1, A_upper = -eps(), V_lower = eps()))\n\nNegative binomial EPCA.\n\nArguments\n\nindim::Integer: The dimension of the input space.\noutdim::Integer: The dimension of the latent (output) space.\nr::Integer: A known parameter of the negative binomial distribution representing the number of successes (positive).\noptions::Options: Optional configuration parameters for the EPCA model. \nA_init_value: Initial fill value for matrix A (default: -1).\nA_upper: The upper bound for the matrix A, default is -eps().\nV_lower: The lower bound for the matrix V, default is eps().\n\ntip: Tip\nTry using options = NegativeDomain() if you encounter domain errors when calling fit! or compress.\n\nReturns\n\nepca: An instance of an EPCA subtype.\n\n\n\n\n\n","category":"function"},{"location":"math/appendix/gamma/#Gamma-EPCA-and-the-Itakura-Saito-Distance","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"","category":"section"},{"location":"math/appendix/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"The cumulant of the gamma distribution is G(theta) = -log(-theta), so the the link function (its derivative) is g(theta) = nabla_theta G(theta) = -frac1theta. From the appendix, we know that f(x) = g^-1(x) = -frac1x and ","category":"page"},{"location":"math/appendix/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"beginaligned\nF(x) \n= theta cdot x - G(theta) \n= f(x) cdot x - G(f(x)) \n= -1 - log(x)\nendaligned","category":"page"},{"location":"math/appendix/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"The Bregman divergence induced from F is","category":"page"},{"location":"math/appendix/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"beginaligned\nB_F(p  q) \n= F(p) - F(q) - langle f(q) p - q rangle \n= -1 - log p + 1 + log q + Biglangle frac1q p - q Bigrangle \n= fracpq - log fracpq - 1\nendaligned","category":"page"},{"location":"math/appendix/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"so B_F is the Itakura-Saito (Itakura and Saito, 1968) distance as desired. Further, the EPCA objective is","category":"page"},{"location":"math/appendix/gamma/","page":"Gamma EPCA and the Itakura-Saito Distance","title":"Gamma EPCA and the Itakura-Saito Distance","text":"beginaligned\nB_F(x  g(theta)) = fracpg(theta) - log fracpg(theta) - 1 = -ptheta - log(-ptheta) - 1\nendaligned","category":"page"},{"location":"constructors/pareto/#Pareto-EPCA","page":"Pareto","title":"Pareto EPCA","text":"","category":"section"},{"location":"constructors/pareto/#Math","page":"Pareto","title":"Math","text":"","category":"section"},{"location":"constructors/pareto/","page":"Pareto","title":"Pareto","text":"Name ParetoEPCA\nG(theta) -log(-1 - theta) + theta log m\ng(theta) log m - frac1theta + 1\nmu Space[1] mathbbR setminus  logm \nTheta Space negative\nAppropriate Data heavy-tail\nm m  0 (minimum value)","category":"page"},{"location":"constructors/pareto/","page":"Pareto","title":"Pareto","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/pareto/#Documentation","page":"Pareto","title":"Documentation","text":"","category":"section"},{"location":"constructors/pareto/#ExpFamilyPCA.ParetoEPCA","page":"Pareto","title":"ExpFamilyPCA.ParetoEPCA","text":"ParetoEPCA(indim::Integer, outdim::Integer, m::Real; options::Options = Options(μ = 2, A_init_value = 2, A_lower = 1 / indim, V_init_value = -2, V_upper = -1))\n\nPareto EPCA.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\nm::Real: A known parameter of the Pareto distribution representing the minimum value in the support.\noptions::Options: Optional parameters for model initialization:\nμ: Default value 2.\nA_init_value: Initial value for matrix A (default: 2).\nA_lower: Lower bound for matrix A (default: 1 / indim).\nV_init_value: Initial value for matrix V (default: -2).\nV_upper: Upper bound for matrix V (default: -1).\n\nReturns\n\nepca: An EPCA subtype for the Pareto distribution.\n\n\n\n\n\n","category":"function"},{"location":"constructors/pareto/","page":"Pareto","title":"Pareto","text":"tip: Tip\nIf your compression converges to a constant matrix, try processing your data to reduce the maximum (e.g., divide your data by a large constant, take the logarithm).","category":"page"},{"location":"api/#API-Documentation","page":"API Documentation","title":"API Documentation","text":"","category":"section"},{"location":"api/#Contents","page":"API Documentation","title":"Contents","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Index","page":"API Documentation","title":"Index","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Functions","page":"API Documentation","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API Documentation","title":"API Documentation","text":"The core of the ExpFamilyPCA.jl API is the EPCA abstract type. All supported and custom EPCA specifications are subtypes of EPCA and include three methods in their EPCA interface: fit!, compress and decompress.","category":"page"},{"location":"api/#ExpFamilyPCA.EPCA","page":"API Documentation","title":"ExpFamilyPCA.EPCA","text":"Supertype for exponential family principal component analysis models.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExpFamilyPCA.fit!","page":"API Documentation","title":"ExpFamilyPCA.fit!","text":"fit!(epca::EPCA, X::AbstractMatrix{T}; maxiter::Integer = 100, verbose::Bool = false, steps_per_print::Integer = 10) where T <: Real\n\nFits the EPCA model on the dataset X. Call this function after creating an EPCA struct or to continue training on more data.\n\nShould be called after creating an EPCA object or when you want to fit on new data.\n\nwarning: Warning\nThe default fit! may have long run times depending on the dataset size and model complexity. Consider adjusting the verbosity (verbose) and number of iterations (maxiter) to better balance runtime and model performance.\n\nArguments\n\nepca::EPCA: The EPCA model. \nX::AbstractMatrix{T}: (n, indim) - The input training data matrix. Rows are observations. Columns are features or variables. \n\nKeyword Arguments\n\nmaxiter::Integer = 100: The maximum number of iterations performed during loss minimization. Defaults to 100. May converge early.\nverbose::Bool = false: A flag indicating whether to print optimization progress. If set to true, prints the loss value and iteration number at specified intervals (steps_per_print). Defaults to false.\nsteps_per_print::Integer = 10: The number of iterations between printed progress updates when verbose is set to true. For example, if steps_per_print is 10, progress will be printed every 10 iterations. Defaults to 10.\n\nReturns\n\nA::AbstractMatrix{T}: (n, outdim) - The compressed data.\n\nUsage\n\nInput:\n\nusing ExpFamilyPCA\nusing Random; Random.seed!(1)\n\n# Create the model\nindim = 10  # Input dimension\noutdim = 5  # Output dimension\nepca = BernoulliEPCA(indim, outdim)\n\n# Generate some random training data\nn = 100\nX = rand(0:1, n, indim)\n\n# Fit the model to the data\nA = fit!(epca, X; maxiter=200, verbose=true, steps_per_print=50);\n\nOutput:\n\nIteration: 1/200 | Loss: 31.7721864082419\nIteration: 50/200 | Loss: 11.07389383509631\nIteration: 100/200 | Loss: 10.971490262772905\nIteration: 150/200 | Loss: 10.886018474442618\nIteration: 200/200 | Loss: 10.718703556787007\n\n\n\n\n\n","category":"function"},{"location":"api/#ExpFamilyPCA.compress","page":"API Documentation","title":"ExpFamilyPCA.compress","text":"compress(epca::EPCA, X::AbstractMatrix{T}; maxiter::Integer = 100, verbose::Bool = false, steps_per_print::Integer = 10) where T <: Real\n\nCompresses the input data X with the EPCA model.\n\nArguments\n\nepca::EPCA: The fitted EPCA model.[1] fit! should be called before compress.\nX::AbstractMatrix{T}: (n, indim) - The input data matrix (can differ from the training data). Rows are observations. Columns are features or variables. \n\nKeyword Arguments\n\nmaxiter::Integer = 100: The maximum number of iterations performed during loss minimization. Defaults to 100. May converge early.\nverbose::Bool = false: A flag indicating whether to print optimization progress. If set to true, prints the loss value and iteration number at specified intervals (steps_per_print). Defaults to false.\nsteps_per_print::Integer = 10: The number of iterations between printed progress updates when verbose is set to true. For example, if steps_per_print is 10, progress will be printed every 10 iterations. Defaults to 10.\n\nReturns\n\nA::AbstractMatrix{T}: (n, outdim) - The compressed data.\n\nUsage\n\n# Generate some random test data\nm = 10\nY = rand(0:1, m, indim)\n\n# Compress the test data using the fitted model from the previous example\nY_compressed = compress(epca, Y)\n\n[1]: If compress is called before fit!, X will compressed using unfitted starting weights.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExpFamilyPCA.decompress","page":"API Documentation","title":"ExpFamilyPCA.decompress","text":"decompress(epca::EPCA, A::AbstractMatrix{T}) where T <: Real\n\nDecompress the compressed matrix A with the EPCA model.\n\nArguments\n\nepca::EPCA: The fitted EPCA model.[1] fit! should be called before compress.\nA::AbstractMatrix{T}: (n, outdim) - A compressed data matrix.\n\nReturns\n\nX̂::AbstractMatrix{T}: (n, indim) - The reconstructed data matrix approximated using EPCA model parameters.\n\nUsage\n\nY_reconstructed = decompress(epca, Y)\n\n\n\n\n\n","category":"function"},{"location":"api/#Options","page":"API Documentation","title":"Options","text":"","category":"section"},{"location":"api/#ExpFamilyPCA.Options","page":"API Documentation","title":"ExpFamilyPCA.Options","text":"Options(; metaprogramming::Bool = true, μ::Real = 1, ϵ::Real = eps(), A_init_value::Real = 1.0, A_lower::Union{Real, Nothing} = nothing, A_upper::Union{Real, Nothing} = nothing, A_use_sobol::Bool = false, V_init_value::Real = 1.0, V_lower::Union{Real, Nothing} = nothing, V_upper::Union{Real, Nothing} = nothing, V_use_sobol::Bool = false, low = -1e10, high = 1e10, tol = 1e-10, maxiter = 1e6)\n\nDefines a struct Options for configuring various parameters used in optimization and calculus. It provides flexible defaults for metaprogramming, initialization values, optimization boundaries, and binary search controls.\n\nFields\n\nmetaprogramming::Bool: Enables metaprogramming for symbolic calculus conversions. Default is true.\nμ::Real: A regularization hyperparameter. Default is 1.\nϵ::Real: A regularization hyperparameter. Default is eps().\nA_init_value::Real: Initial value for parameter A. Default is 1.0.\nA_lower::Union{Real, Nothing}: Lower bound for A, or nothing. Default is nothing.\nA_upper::Union{Real, Nothing}: Upper bound for A, or nothing. Default is nothing.\nA_use_sobol::Bool: Use Sobol sequences for initializing A. Default is false.\nV_init_value::Real: Initial value for parameter V. Default is 1.0.\nV_lower::Union{Real, Nothing}: Lower bound for V, or nothing. Default is nothing.\nV_upper::Union{Real, Nothing}: Upper bound for V, or nothing. Default is nothing.\nV_use_sobol::Bool: Use Sobol sequences for initializing V. Default is false.\nlow::Real: Lower bound for binary search. Default is -1e10.\nhigh::Real: Upper bound for binary search. Default is 1e10.\ntol::Real: Tolerance for stopping binary search. Default is 1e-10.\nmaxiter::Real: Maximum iterations for binary search. Default is 1e6.\n\ninfo: Info\nThe metaprogramming flag controls whether metaprogramming is used during symbolic differentiation conversion. While conversion between Symbolics.jl atoms and base Julia can occur without it, this approach is slower and requires more calls. Nonetheless, the flag is provided for users who keenly want to avoid metaprogramming in their pipeline.\n\n\n\n\n\n","category":"type"},{"location":"api/#ExpFamilyPCA.NegativeDomain","page":"API Documentation","title":"ExpFamilyPCA.NegativeDomain","text":"NegativeDomain(; metaprogramming::Bool = true, μ::Real = 1, ϵ::Real = eps(), low::Real = -1e10, high::Real = 1e10, tol::Real = 1e-10, maxiter::Real = 1e6)\n\nReturns an instance of Options configured for optimization over the negative domain. Sets defaults for A and V parameters while keeping the remaining settings from Options.\n\nSpecific Settings\n\nA_init_value = -1: Initializes A with a negative value.\nA_upper = -1e-4: Upper bound for A is constrained to a small negative value.\nV_init_value = 1: Initializes V with a positive value.\nV_lower = 1e-4: Lower bound for V is constrained to a small positive value.\n\nOther fields inherit from the Options struct.\n\n\n\n\n\n","category":"function"},{"location":"api/#ExpFamilyPCA.PositiveDomain","page":"API Documentation","title":"ExpFamilyPCA.PositiveDomain","text":"PositiveDomain(; metaprogramming::Bool = true, μ::Real = 1, ϵ::Real = eps(), low::Real = -1e10, high::Real = 1e10, tol::Real = 1e-10, maxiter::Real = 1e6)\n\nReturns an instance of Options configured for optimization over the positive domain. Sets defaults for A and V parameters while keeping the remaining settings from Options.\n\nSpecific Settings\n\nA_init_value = 1: Initializes A with a positive value.\nA_lower = 1e-4: Lower bound for A is constrained to a small positive value.\nV_init_value = 1: Initializes V with a positive value.\nV_lower = 1e-4: Lower bound for V is constrained to a small positive value.\n\nOther fields inherit from the Options struct.\n\n\n\n\n\n","category":"function"},{"location":"constructors/bernoulli/#Bernoulli-EPCA","page":"Bernoulli","title":"Bernoulli EPCA","text":"","category":"section"},{"location":"constructors/bernoulli/#Math","page":"Bernoulli","title":"Math","text":"","category":"section"},{"location":"constructors/bernoulli/","page":"Bernoulli","title":"Bernoulli","text":"Name BernoulliEPCA\nG(theta) log(1 + e^theta)\ng(theta) frace^theta1+e^theta\nmu Space[1] (0 1)\nTheta Space real\nAppropriate Data binary","category":"page"},{"location":"constructors/bernoulli/","page":"Bernoulli","title":"Bernoulli","text":"G is the softplus function and g is the logistic function.","category":"page"},{"location":"constructors/bernoulli/","page":"Bernoulli","title":"Bernoulli","text":"[1]: mu space refers to the space of valid regularization parameters, not to the expectation parameter space.","category":"page"},{"location":"constructors/bernoulli/#Documentation","page":"Bernoulli","title":"Documentation","text":"","category":"section"},{"location":"constructors/bernoulli/#ExpFamilyPCA.BernoulliEPCA","page":"Bernoulli","title":"ExpFamilyPCA.BernoulliEPCA","text":"BernoulliEPCA(indim::Integer, outdim::Integer; options = Options(μ = 0.5))\n\nBernoulli EPCA.\n\nArguments\n\nindim::Integer: Dimension of the input space.\noutdim::Integer: Dimension of the latent (output) space.\noptions::Options: Optional parameters (default: μ = 0.5).\n\nReturns\n\nepca: An EPCA subtype for the Bernoulli distribution.\n\n\n\n\n\n","category":"function"},{"location":"math/objectives/#Deriving-the-EPCA-Objective-Function","page":"EPCA Objectives","title":"Deriving the EPCA Objective Function","text":"","category":"section"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"When working with custom distributions, certain specifications are often more convenient and computationally efficient than others. For example, expressing the log-partition function of the gamma distribution as G(theta) = -log(-theta) and its derivative g(theta) = -1theta is significantly simpler than implementing the Itakura-Saito distance (Itakura and Saito, 1968):","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"D(P(omega) hatP(omega)) = frac12pi int_-pi^pi Bigg fracP(omega)hatP(omega) - log fracP(omega)hatPomega - 1Bigg  domega","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"even though both formulations are equivalent (see the appendix). This example highlights the importance of flexibility in specifying mathematical components when the EPCA model. Choosing convenient representations is simpler and sometimes more effecient.","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"In this section, we demonstrate how the EPCA objective function and the decompression function g can be derived using different combinations of mathematical components. This flexibility allows for efficient and adaptable implementations of EPCA in Julia.","category":"page"},{"location":"math/objectives/#The-Regularized-EPCA-Objective","page":"EPCA Objectives","title":"The Regularized EPCA Objective","text":"","category":"section"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Recall from the introduction that the regularized EPCA objective aims to minimize the following expression:","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"B_F(X  g(Theta)) + epsilon B_F(mu_0  g(Theta))","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"where:","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"B_F is the Bregman divergence generated by a convex and continuously differentiable function F,\nF is the convex conjugate of G,\nG is the log-parition function (or any convex function),\ng is the link function,\nX is the data matrix,\nTheta is the parameter matrix, and\nmu_0 in mathrmrange(g) and epsilon  0 are regularization parameters.","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Our goal is to show that both B_F and g can be induced from various base components, namely F, G, f and g. This allows for multiple pathways to define and compute the EPCA objective in Julia.","category":"page"},{"location":"math/objectives/#Different-Approaches-to-Specifying-the-EPCA-Objective-and-Decompression","page":"EPCA Objectives","title":"Different Approaches to Specifying the EPCA Objective and Decompression","text":"","category":"section"},{"location":"math/objectives/#1.-Using-F-and-g","page":"EPCA Objectives","title":"1. Using F and g","text":"","category":"section"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"We begin by showing that the convex function F and the link function g are sufficient to define the EPCA objective. The Bregman divergence B_F(X  g(Theta)) can be expressed as","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"beginaligned\nB_F(X  g(Theta)) = F(X) - F(g(Theta)) - f(g(Theta))(X - g(Theta)) \n= F(X) - F(g(Theta)) - Theta(X - g(Theta))\nendaligned","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"where the second line follows from the relationship f = g^-1 (see the appendix) and Theta = f(g(Theta)). Thus, specifying F and g is sufficient to fully describe the EPCA objective and decompression.","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Example:","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"F(x) = x * log(x) - x\ng(θ) = exp(θ)\npoisson_epca = EPCA(indim, outdim, F, g, Val((:F, :g)))","category":"page"},{"location":"math/objectives/#2.-Using-F-and-f","page":"EPCA Objectives","title":"2. Using F and f","text":"","category":"section"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Since g can be recovered from f (the inverse of g), we can also derive the EPCA objective using F and f. Given that g is strictly increasing (as G, the conjugate of F, is strictly convex and differentiable), we can compute g numerically.","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"To evaluate g(a) for any a in the domain of g, we solve for x in the equation f(x) = a. Since f and g are monotone, we can effeciently solve for x by finding the unique root of f(a) - x using a binary search. Therefore, the EPCA objective can also be specified using only F and f.","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Example:","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"F(x) = x * log(x) - x\nf(x) = log(x + eps())\npoisson_epca = EPCA(indim, outdim, F, f, Val((:F, :f)))","category":"page"},{"location":"math/objectives/#3.-Using-F-Alone","page":"EPCA Objectives","title":"3. Using F Alone","text":"","category":"section"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"ExpFamilyPCA.jl's versatility is a direct result of two properties of Julia. The first is multiple dispatch. Julia's multiple dispatch system promotes high levels of generic code reuse (Karpinski, 2019) meaning libraries used for symbolic differentiation like Symbolics.jl (Gowda et al., 2022) can return base Julia atoms that work well with optimization libraries like Optim.jl (Mogensen and Riseth, 2018).","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"To induce the EPCA objective from F alone, we first use Symbolics.jl to recover f. We convert f to base Julia's second useful property: metaprogramming (non-metaprogramming conversion is available, though it generally performs much slower). Since both F and f are represented using generic Julia code, they seamlessly integrate with Optim.jl.","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"To recover g for decompression, we can use the same procedure as described above (which is again possible because of multiple dispatch). Therefore, by defining F alone, we can induce all necessary components to specify and compute the EPCA objective and decompression.","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Example:","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"F(x) = x * log(x) - x\npoisson_epca = EPCA(indim, outdim, F, Val((:F)))","category":"page"},{"location":"math/objectives/#4.-Using-G-and-g","page":"EPCA Objectives","title":"4. Using G and g","text":"","category":"section"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Alternatively, we can express the EPCA objective using the log-partition function G and the link function g. Starting from the first dervition (and dropping the constant), we have:","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"beginaligned\n-F(g(Theta)) - Theta(X - g(Theta)) = G(Theta) - g(Theta) Theta - Theta(X - g(Theta)) \n= G(Theta) - Theta X\nendaligned","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"This shows that the EPCA objective simplifies to the negative log-likelihood G(Theta) - Theta X. Therefore, G and g are sufficient to define the EPCA objecitve and link function.","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Example:","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"G(θ) = exp(θ)\ng(θ) = exp(θ)\npoisson_epca = EPCA(indim, outdim, G, g, Val((:G, :g)))","category":"page"},{"location":"math/objectives/#5.-Using-G-Alone","page":"EPCA Objectives","title":"5. Using G Alone","text":"","category":"section"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Since g is the derivative of G (i.e., g = G), we can now recover g directly from G via symbolic differentiation. This means that providing G alone is enough to specify both the EPCA objective and the link function g.","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Example:","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"G(θ) = exp(θ)\npoisson_epca = EPCA(indim, outdim, G, Val((:G)))","category":"page"},{"location":"math/objectives/#6.-Using-B_F-and-g","page":"EPCA Objectives","title":"6. Using B_F and g","text":"","category":"section"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"If we already have the Bregman divergence B_F and the link function g, specifying the EPCA objective is trivial.","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Example:","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"using Distances\n\nB = Distances.gkl_divergence\ng(θ) = exp(θ)\npoisson_epca = EPCA(indim, outdim, B, g, Val((:B, :g)))","category":"page"},{"location":"math/objectives/#7.-Using-\\tilde{B}-and-g","page":"EPCA Objectives","title":"7. Using tildeB and g","text":"","category":"section"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Similarly, using the transformed Bregman divergence tildeB(p q) = B_F(p  g(q)) along with g is straightforward. Given that tildeB is just B_F evaluated at g(q), and we already have the link function g, defining the EPCA objective is almost too obvious to mention. While mathematically plain, this specification is useful in practice to avoid domain errors that make optimization difficult with certain exponential family members (e.g., gamma, negative binomial, Pareto).","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Example:","category":"page"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"Bg(x, θ) = exp(θ) - x * θ + x * log(x) - x\ng(θ) = exp(θ)\npoisson_epca = EPCA(indim, outdim, Bg, g, Val((:Bg, :g)))","category":"page"},{"location":"math/objectives/#Conclusion","page":"EPCA Objectives","title":"Conclusion","text":"","category":"section"},{"location":"math/objectives/","page":"EPCA Objectives","title":"EPCA Objectives","text":"In summary, the EPCA objective function and the decompression function g can be derived from various components. The flexibility afforded by Julia's multiple dispatch and symbolic differentiation capabilities allows for efficient computation of EPCA in different scenarios. Depending on the form of the data and the problem at hand, one can choose the most convenient set of components to define and compute the EPCA objective.","category":"page"},{"location":"math/appendix/expectation/#The-Link-Function-and-the-Expectation-Parameter","page":"Link Functions and Expectations","title":"The Link Function and the Expectation Parameter","text":"","category":"section"},{"location":"math/appendix/expectation/","page":"Link Functions and Expectations","title":"Link Functions and Expectations","text":"Recall from the page on Bregman divergences that the probability density function for a member of the natural exponential family is given by","category":"page"},{"location":"math/appendix/expectation/","page":"Link Functions and Expectations","title":"Link Functions and Expectations","text":"p_theta(x) = h(x) exp(x theta - G(theta))","category":"page"},{"location":"math/appendix/expectation/","page":"Link Functions and Expectations","title":"Link Functions and Expectations","text":"where G(theta) is the log-partition function, defined as","category":"page"},{"location":"math/appendix/expectation/","page":"Link Functions and Expectations","title":"Link Functions and Expectations","text":"G(theta) = log int h(x) exp(xtheta)  dx","category":"page"},{"location":"math/appendix/expectation/","page":"Link Functions and Expectations","title":"Link Functions and Expectations","text":"Now, by taking the gradient of the log-partition function G(theta), we get:","category":"page"},{"location":"math/appendix/expectation/","page":"Link Functions and Expectations","title":"Link Functions and Expectations","text":"beginaligned\nnabla_theta G(theta) \n= nabla_theta left log int h(x) exp(x theta)  dx right \n= frac int x exp(x theta) h(x)  dx int exp(x theta) h(x)  dx \n= frac int x exp(x theta) h(x)  dx exp(G(theta)) \n= int x exp(x theta - G(theta)) h(x)  dx \n= int x p_theta(x)  dx \n= mathbbE_thetaX\nendaligned","category":"page"},{"location":"math/intro/#Introduction","page":"Introduction","title":"Introduction","text":"","category":"section"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"Welcome to the first of three pages exploring exponential family principal component analysis (EPCA) (Collins et al., 2001). EPCA is a robust dimensionality reduction technique that extends traditional principal component analysis (PCA) (Pearson, 1901) by providing a probabilistic framework capable of handling complex, high-dimensional data with greater flexibility. Whether you are a data scientist, machine learning researcher, or mathematician, this guide will offer an accessible yet rigorous look into EPCA, building on foundational knowledge of multivariable calculus and linear algebra.","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"In this introduction, we’ll provide an overview of the foundational math and motivation behind EPCA. While advanced mathematical concepts will be covered, I aim to present them in a way that's approachable, even for readers less familiar with the subtleties of these topics. For a more concise and formal treatment, please refer to the original EPCA paper.","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"This guide is organized into several pages, each building on the last:","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"Introduction to EPCA (this page)\nBregman Divergences\nEPCA Objectives and Derivations\nAppendix\nThe Gamma EPCA Objective is the Itakura-Saito Distance\nThe Poisson EPCA Objective is the Generalized KL-Divergence\nThe Gaussian EPCA Objective is the PCA Objective\nThe Inverse of the Link Function is the Gradient of the Convex Conjugate\nThe Link Function and the Expectation Parameter\nReferences","category":"page"},{"location":"math/intro/#Principal-Component-Analysis-(PCA)","page":"Introduction","title":"Principal Component Analysis (PCA)","text":"","category":"section"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"PCA is one of the most widely used dimension reduction techniques, particularly in data science and machine learning. It transforms high-dimensional data into a lower-dimensional space while retaining as much of the data’s variability as possible. By identifying directions of maximum variance—known as principal components—PCA provides an efficient way to project the data onto these new bases, making it easier to visualize, analyze, and interpret.","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"Applications of PCA are broad, from noise reduction and feature extraction to exploratory data analysis and visualization of complex datasets. In this section, we will focus on two primary interpretations of PCA: a geometric view and a probabilistic approach that links PCA with Gaussian noise reduction.","category":"page"},{"location":"math/intro/#Geometric-Interpretation","page":"Introduction","title":"Geometric Interpretation","text":"","category":"section"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"The geometric interpretation of PCA is intuitive and grounded in linear algebra. Given a high-dimensional dataset X in mathbbR^n times d, the goal is to find a lower-dimensional representation that best approximates the original data. PCA seeks the closest low-dimensional subspace by minimizing the distance between the data and its projection onto this subspace.","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"This can be formulated as an optimization problem where we find the rank-k approximation Theta of the data matrix X by minimizing the reconstruction error:","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"beginaligned\n undersetThetatextminimize\n  X - Theta_F^2 \n textsubject to\n  mathrmrankleft(Thetaright) = k\nendaligned","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"where  cdot _F denotes the Frobenius norm. The Frobenius norm is calculated as the square root of the sum of the squared differences between corresponding elements of the two matrices:","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":" X - Theta _F^2 = sum_i=1^nsum_j=1^d(X_ij-Theta_ij)^2","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"Intuitively, it can be seen as an extension of the Euclidean distance for vectors, applied to matrices by flattening them into large vectors. This makes the Frobenius norm a natural way to measure how well the lower-dimensional representation approximates the original data.","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"To find the approximation, we decompose Theta into a product Theta = AV where A in mathbbR^n times k contains the projected data in the lower-dimensional space, and V in mathbbR^k times d is the matrix of principal component, which define the new orthogonal axes. ","category":"page"},{"location":"math/intro/#Probabilistic-Interpretation","page":"Introduction","title":"Probabilistic Interpretation","text":"","category":"section"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"In addition to its geometric interpretation, PCA can also be understood from a probabilistic perspective, particularly when the data is assumed to be corrupted by Gaussian noise. From this viewpoint, PCA is not just about finding a low-dimensional subspace, but about recovering the most likely low-dimensional structure underlying noisy high-dimensional observations.","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"In the probabilistic formulation, we assume that each observed data point x_i in mathbbR^d is a random sample from a Gaussian with mean theta_i in mathbbR^k and unit covariance: ","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"x_i sim mathcalN(theta_i I)","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"The goal of PCA here is to find the parameters Theta = theta_1 dots theta_n that maximize the likelihood of observing the data under the Gaussian model. Maximizing the log-likelihood for this Gaussian model leads to the following expression:","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"ell(Theta X) = frac12 sum_i=1^n (x_i-theta_i)^2","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"which is equivalent to minimizing the squared Frobenius norm in the geometric interpretation.","category":"page"},{"location":"math/intro/#Exponential-Family-PCA-(EPCA)","page":"Introduction","title":"Exponential Family PCA (EPCA)","text":"","category":"section"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"EPCA is similar to generalized linear models (GLMs) (McCullagh and Nelder, 1989). Just as GLMs extend linear regression to handle a variety of response distributions, EPCA generalizes PCA to accommodate data with noise drawn from any exponential family distribution, rather than just Gaussian noise. This allows EPCA to address a broader range of real-world data scenarios where the Gaussian assumption may not hold (e.g., binary, count, discrete distribution data).","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"At its core, EPCA replaces the geometric PCA objective with a more general probabilistic objective that minimizes the generalized Bregman divergence—a measure closely related to the exponential family—rather than the squared Frobenius norm, which PCA uses. This makes EPCA particularly versatile for dimensionality reduction when working with non-Gaussian data distributions:","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"beginaligned\n undersetThetatextminimize\n  B_F(X  g(Theta)) + epsilon B_F(mu_0  g(Theta)) \n textsubject to\n  mathrmrankleft(Thetaright) = k\nendaligned","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"In this formulation:","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"g(theta) is the link function and the derivative of G,\nF(mu) is the convex conjugate or dual of G,\nB_F(p  q) is the Bregman divergence induced from F,\nand both mu_0 in mathrmrange(g) and epsilon  0 are regularization hyperparameters.","category":"page"},{"location":"math/intro/","page":"Introduction","title":"Introduction","text":"On the next page, we dive deeper into Bregman divergences. We’ll explore their properties and how they connect to the exponential family, providing a solid foundation for understanding the probabilistic framework of EPCA. ","category":"page"},{"location":"#ExpFamilyPCA.jl-Documentation","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl Documentation","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"ExpFamilyPCA.jl is a Julia package for performing exponential principal component analysis (EPCA) (Collins et al., 2001). EPCA generalizes PCA to accommodate data from any exponential family distribution, making it more suitable for fields where these data types are common, such as geochemistry, marketing, genomics, political science, and machine learning (Greenacre, 2021; Hastie et al., 2009).","category":"page"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"ExpFamilyPCA.jl the first EPCA package in Julia and the first in any language to support EPCA for multiple distributions and custom objectives.","category":"page"},{"location":"#Installation","page":"ExpFamilyPCA.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"To install the package, use the Julia package manager. In the Julia REPL, type:","category":"page"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"using Pkg; Pkg.add(\"ExpFamilyPCA\")","category":"page"},{"location":"#Quickstart","page":"ExpFamilyPCA.jl","title":"Quickstart","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"using ExpFamilyPCA\n\nindim = 5\nX = rand(1:100, (10, indim))  # data matrix to compress\noutdim = 3  # target compression dimension\n\npoisson_epca = PoissonEPCA(indim, outdim)\n\nX_compressed = fit!(poisson_epca, X; maxiter=200, verbose=true)\n\nY = rand(1:100, (3, indim))  # test data\nY_compressed = compress(poisson_epca, Y; maxiter=200, verbose=true)\n\nX_reconstructed = decompress(poisson_epca, X_compressed)\nY_reconstructed = decompress(poisson_epca, Y_compressed)","category":"page"},{"location":"#Supported-Distributions","page":"ExpFamilyPCA.jl","title":"Supported Distributions","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"Distribution ExpFamilyPCA.jl Objective Link Function g(theta)\nBernoulli BernoulliEPCA log(1 + e^theta - 2xtheta) frace^theta1 + e^theta\nBinomial BinomialEPCA n log(1 + e^theta) - xtheta fracne^theta1 + e^theta\nContinuous Bernoulli ContinuousBernoulliEPCA logleft(frace^theta - 1thetaright) - xtheta fractheta - 1theta + frac1e^theta - 1\nGamma¹ GammaEPCA or ItakuraSaitoEPCA -log(-theta) - xtheta -frac1theta\nGaussian² GaussianEPCA or NormalEPCA frac12(x - theta)^2 theta\nNegative Binomial NegativeBinomialEPCA -r log(1 - e^theta) - xtheta frac-re^thetae^theta - 1\nPareto ParetoEPCA -log(-1 - theta) + theta log m - xtheta log m - frac1theta + 1\nPoisson³ PoissonEPCA e^theta - xtheta e^theta\nWeibull WeibullEPCA -log(-theta) - xtheta -frac1theta","category":"page"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"¹: For the Gamma distribution, the link function is typically based on the inverse relationship.  ","category":"page"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"²: For Gaussian, also known as Normal distribution, the link function is the identity. ","category":"page"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"³: The Poisson distribution link function is exponential.","category":"page"},{"location":"#Contributing","page":"ExpFamilyPCA.jl","title":"Contributing","text":"","category":"section"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"ExpFamilyPCA.jl was designed through the Stanford Intelligent Systems Laboratory (SISL) which helps maintain open-source projects posted on the SISL organization repository and the JuliaPOMDPs community. We subscribe to the community's contribution guidelines and encourage new users with all levels of software development experience to contribute and ask questions. ","category":"page"},{"location":"","page":"ExpFamilyPCA.jl","title":"ExpFamilyPCA.jl","text":"If you would like to create a new compressor or improve an existing model, please open a new issue that briefly describes the contribution you would like to make and the problem that it fixes, if there is one.","category":"page"}]
}

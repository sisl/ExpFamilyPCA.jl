<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Introduction · ExpFamilyPCA</title><meta name="title" content="Introduction · ExpFamilyPCA"/><meta property="og:title" content="Introduction · ExpFamilyPCA"/><meta property="twitter:title" content="Introduction · ExpFamilyPCA"/><meta name="description" content="Documentation for ExpFamilyPCA."/><meta property="og:description" content="Documentation for ExpFamilyPCA."/><meta property="twitter:description" content="Documentation for ExpFamilyPCA."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">ExpFamilyPCA</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">ExpFamilyPCA.jl</a></li><li><span class="tocitem">Math</span><ul><li class="is-active"><a class="tocitem" href>Introduction</a><ul class="internal"><li><a class="tocitem" href="#Principal-Component-Analysis-(PCA)"><span>Principal Component Analysis (PCA)</span></a></li><li><a class="tocitem" href="#Exponential-Family-PCA-(EPCA)"><span>Exponential Family PCA (EPCA)</span></a></li></ul></li><li><a class="tocitem" href="../bregman/">Bregman Divergences</a></li><li><a class="tocitem" href="../objectives/">EPCA Objectives</a></li><li><input class="collapse-toggle" id="menuitem-2-4" type="checkbox"/><label class="tocitem" for="menuitem-2-4"><span class="docs-label">Appendix</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../appendix/gamma/">Gamma EPCA and the Itakura-Saito Distance</a></li><li><a class="tocitem" href="../appendix/poisson/">Poisson EPCA and Generalized KL-Divergence</a></li><li><a class="tocitem" href="../appendix/gaussian/">Gaussian EPCA and the Squared Frobenius Norm</a></li><li><a class="tocitem" href="../appendix/inverses/">Inverse Link Functions</a></li><li><a class="tocitem" href="../appendix/expectation/">Link Functions and Expectations</a></li></ul></li><li><a class="tocitem" href="../references/">References</a></li></ul></li><li><span class="tocitem">Constructors</span><ul><li><a class="tocitem" href="../../constructors/bernoulli/">Bernoulli</a></li><li><a class="tocitem" href="../../constructors/binomial/">Binomial</a></li><li><a class="tocitem" href="../../constructors/continuous_bernoulli/">Continuous Bernoulli</a></li><li><a class="tocitem" href="../../constructors/gamma/">Gamma</a></li><li><a class="tocitem" href="../../constructors/gaussian/">Gaussian</a></li><li><a class="tocitem" href="../../constructors/negative_binomial/">Negative Binomial</a></li><li><a class="tocitem" href="../../constructors/pareto/">Pareto</a></li><li><a class="tocitem" href="../../constructors/poisson/">Poisson</a></li><li><a class="tocitem" href="../../constructors/weibull/">Weibull</a></li></ul></li><li><a class="tocitem" href="../../api/">API Documentation</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Math</a></li><li class="is-active"><a href>Introduction</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Introduction</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/sisl/ExpFamilyPCA.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/sisl/ExpFamilyPCA.jl/blob/main/docs/src/math/intro.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Introduction"><a class="docs-heading-anchor" href="#Introduction">Introduction</a><a id="Introduction-1"></a><a class="docs-heading-anchor-permalink" href="#Introduction" title="Permalink"></a></h1><p>Welcome to the first of three pages exploring exponential family principal component analysis (EPCA) (<a href="../references/#EPCA">Collins <em>et al.</em>, 2001</a>). EPCA is a robust dimensionality reduction technique that extends traditional principal component analysis (PCA) (<a href="../references/#PCA">Pearson, 1901</a>) by providing a probabilistic framework capable of handling complex, high-dimensional data with greater flexibility. Whether you are a data scientist, machine learning researcher, or mathematician, this guide will offer an accessible yet rigorous look into EPCA, building on foundational knowledge of multivariable calculus and linear algebra.</p><p>In this introduction, we’ll provide an overview of the foundational math and motivation behind EPCA. While advanced mathematical concepts will be covered, I aim to present them in a way that&#39;s approachable, even for readers less familiar with the subtleties of these topics. For a more concise and formal treatment, please refer to the <a href="../references/#EPCA">original EPCA paper</a>.</p><p>This guide is organized into several pages, each building on the last:</p><ol><li><p>Introduction to EPCA (this page)</p></li><li><p><a href="../bregman/">Bregman Divergences</a></p></li><li><p><a href="../objectives/">EPCA Objectives and Derivations</a></p></li><li><p>Appendix</p><ul><li><a href="../appendix/gamma/">The Gamma EPCA Objective is the Itakura-Saito Distance</a></li><li><a href="../appendix/poisson/">The Poisson EPCA Objective is the Generalized KL-Divergence</a></li><li><a href="../appendix/gaussian/">The Gaussian EPCA Objective is the PCA Objective</a></li><li><a href="../appendix/inverses/">The Inverse of the Link Function is the Gradient of the Convex Conjugate</a></li><li><a href="../appendix/expectation/">The Link Function and the Expectation Parameter</a></li></ul></li><li><p><a href="../references/">References</a></p></li></ol><h2 id="Principal-Component-Analysis-(PCA)"><a class="docs-heading-anchor" href="#Principal-Component-Analysis-(PCA)">Principal Component Analysis (PCA)</a><a id="Principal-Component-Analysis-(PCA)-1"></a><a class="docs-heading-anchor-permalink" href="#Principal-Component-Analysis-(PCA)" title="Permalink"></a></h2><p>PCA is one of the most widely used dimension reduction techniques, particularly in data science and machine learning. It transforms high-dimensional data into a lower-dimensional space while retaining as much of the data’s variability as possible. By identifying directions of maximum variance—known as principal components—PCA provides an efficient way to project the data onto these new bases, making it easier to visualize, analyze, and interpret.</p><p>Applications of PCA are broad, from noise reduction and feature extraction to exploratory data analysis and visualization of complex datasets. In this section, we will focus on two primary interpretations of PCA: a geometric view and a probabilistic approach that links PCA with Gaussian noise reduction.</p><h3 id="Geometric-Interpretation"><a class="docs-heading-anchor" href="#Geometric-Interpretation">Geometric Interpretation</a><a id="Geometric-Interpretation-1"></a><a class="docs-heading-anchor-permalink" href="#Geometric-Interpretation" title="Permalink"></a></h3><p>The geometric interpretation of PCA is intuitive and grounded in linear algebra. Given a high-dimensional dataset <span>$X \in \mathbb{R}^{n \times d}$</span>, the goal is to find a lower-dimensional representation that best approximates the original data. PCA seeks the closest low-dimensional subspace by minimizing the distance between the data and its projection onto this subspace.</p><p>This can be formulated as an optimization problem where we find the rank-<span>$k$</span> approximation <span>$\Theta$</span> of the data matrix <span>$X$</span> by minimizing the reconstruction error:</p><p class="math-container">\[\begin{aligned}
&amp; \underset{\Theta}{\text{minimize}}
&amp; &amp; \|X - \Theta\|_F^2 \\
&amp; \text{subject to}
&amp; &amp; \mathrm{rank}\left(\Theta\right) = k
\end{aligned}\]</p><p>where <span>$\| \cdot \|_F$</span> denotes the Frobenius norm. The Frobenius norm is calculated as the square root of the sum of the squared differences between corresponding elements of the two matrices:</p><p class="math-container">\[\| X - \Theta \|_F^2 = \sum_{i=1}^{n}\sum_{j=1}^{d}(X_{ij}-\Theta_{ij})^2.\]</p><p>Intuitively, it can be seen as an extension of the Euclidean distance for vectors, applied to matrices by flattening them into large vectors. This makes the Frobenius norm a natural way to measure how well the lower-dimensional representation approximates the original data.</p><p>To find the approximation, we decompose <span>$\Theta$</span> into a product <span>$\Theta = AV$</span> where <span>$A \in \mathbb{R}^{n \times k}$</span> contains the projected data in the lower-dimensional space, and <span>$V \in \mathbb{R}^{k \times d}$</span> is the matrix of principal component, which define the new orthogonal axes. </p><h3 id="Probabilistic-Interpretation"><a class="docs-heading-anchor" href="#Probabilistic-Interpretation">Probabilistic Interpretation</a><a id="Probabilistic-Interpretation-1"></a><a class="docs-heading-anchor-permalink" href="#Probabilistic-Interpretation" title="Permalink"></a></h3><p>In addition to its geometric interpretation, PCA can also be understood from a probabilistic perspective, particularly when the data is assumed to be corrupted by Gaussian noise. From this viewpoint, PCA is not just about finding a low-dimensional subspace, but about recovering the most likely low-dimensional structure underlying noisy high-dimensional observations.</p><p>In the probabilistic formulation, we assume that each observed data point <span>$x_i \in \mathbb{R}^{d}$</span> is a random sample from a Gaussian with mean <span>$\theta_i \in \mathbb{R}^{k}$</span> and unit covariance: </p><p class="math-container">\[x_i \sim \mathcal{N}(\theta_i, I).\]</p><p>The goal of PCA here is to find the parameters <span>$\Theta = [\theta_1, \dots, \theta_n]$</span> that maximize the likelihood of observing the data under the Gaussian model. Maximizing the log-likelihood for this Gaussian model leads to the following expression:</p><p class="math-container">\[\ell(\Theta; X) = \frac{1}{2} \sum_{i=1}^{n} (x_i-\theta_i)^2\]</p><p>which is equivalent to minimizing the squared Frobenius norm in the geometric interpretation.</p><h2 id="Exponential-Family-PCA-(EPCA)"><a class="docs-heading-anchor" href="#Exponential-Family-PCA-(EPCA)">Exponential Family PCA (EPCA)</a><a id="Exponential-Family-PCA-(EPCA)-1"></a><a class="docs-heading-anchor-permalink" href="#Exponential-Family-PCA-(EPCA)" title="Permalink"></a></h2><p>EPCA is similar to generalized linear models (GLMs) (<a href="../references/#GLM">McCullagh and Nelder, 1989</a>). Just as GLMs extend linear regression to handle a variety of response distributions, EPCA generalizes PCA to accommodate data with noise drawn from any exponential family distribution, rather than just Gaussian noise. This allows EPCA to address a broader range of real-world data scenarios where the Gaussian assumption may not hold (e.g., binary, count, discrete distribution data).</p><p>At its core, EPCA replaces the geometric PCA objective with a more general probabilistic objective that minimizes the generalized Bregman divergence—a measure closely related to the exponential family—rather than the squared Frobenius norm, which PCA uses. This makes EPCA particularly versatile for dimensionality reduction when working with non-Gaussian data distributions:</p><p class="math-container">\[\begin{aligned}
&amp; \underset{\Theta}{\text{minimize}}
&amp; &amp; B_F(X \| g(\Theta)) + \epsilon B_F(\mu_0 \| g(\Theta)) \\
&amp; \text{subject to}
&amp; &amp; \mathrm{rank}\left(\Theta\right) = k.
\end{aligned}\]</p><p>In this formulation:</p><ul><li><span>$g(\theta)$</span> is the <strong>link function</strong> and the derivative of <span>$G$</span>,</li><li><span>$F(\mu)$</span> is the <strong>convex conjugate</strong> or dual of <span>$G$</span>,</li><li><span>$B_F(p \| q)$</span> is the <strong>Bregman divergence</strong> induced from <span>$F$</span>,</li><li>and both <span>$\mu_0 \in \mathrm{range}(g)$</span> and <span>$\epsilon &gt; 0$</span> are regularization hyperparameters.</li></ul><p>On the <a href="../bregman/">next page</a>, we dive deeper into Bregman divergences. We’ll explore their properties and how they connect to the exponential family, providing a solid foundation for understanding the probabilistic framework of EPCA. </p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../">« ExpFamilyPCA.jl</a><a class="docs-footer-nextpage" href="../bregman/">Bregman Divergences »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.7.0 on <span class="colophon-date" title="Thursday 9 January 2025 19:43">Thursday 9 January 2025</span>. Using Julia version 1.11.2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
